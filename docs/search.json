[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Controlling for Log Age\n\n\n\n\n\nShort blogpost summarizing how and why one would control for the log of the age of a firm\n\n\n\n\n\n06-2020\n\n\n\n\n\n\n\nControlling for Log Age\n\n\n\n\n\nShort blogpost summarizing how and why one would control for the log of the age of a firm\n\n\n\n\n\n06-2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/posts/DiD/index.html",
    "href": "posts/posts/DiD/index.html",
    "title": "Controlling for Log Age",
    "section": "",
    "text": "In doing research for my dissertation I keep on running across models that control for somewhat arbitrary variables (or where at least there is no justification for their inclusion). This is common in applied corporate finance / managerial accounting papers. As a result, I snarked.\n\n\nThe issue here is that in regressions for some outcome - say firm valuation (the dreaded Q) - we want to look at the the change in the outcome variable around some treatment shock, but we want to control for some variables. The big ones here are firm age and firm size, using as a proxy firm market value (yes I realize it makes little sense to regress the firm valuation ratio — Tobin’s Q \\(\\approx\\) market value / book value — on the market value but it’s what people do).\nA problem arises when you use firm fixed effects and period fixed effects, because firm age becomes collinear with the fixed effect structure. As a result researchers will often control for the log of firm value instead, which is not perfectly collinear. This strikes me as very odd, because firm age is still an identity that comes directly from a firm variable (the first year in the panel) and a fixed effect (the period FE). So, I figured I would simulate some data to see if it works as we might hope it does.\nAssume we’re in a setting where the difference-in-differences (DiD) issues that I’ve written about previously here don’t apply - namely that there are many more non-treated units than treated units so that the negative weighting issue identified by Andrew Goodman-Bacon and others does not bite. Assume that we think the data generating process is as follows:\n\\[ y_{it} = \\alpha_i + \\alpha_t + \\delta T_{it} + \\epsilon_{it} \\\\\n\\alpha_i \\sim N(0, 1) \\\\\n\\alpha_t \\sim N(0, 1) \\\\\n\\epsilon_{it} \\sim N(0, 1) \\]\nThat is, our outcome variable for firm \\(i\\) in period \\(t\\) is a linear function of a firm specific effect \\(\\alpha_i\\), a period effect \\(\\alpha_t\\), a treatment effect \\(\\delta\\) and a stochastic error term \\(\\epsilon_{it}\\). Assume that, for some reason, we want to control for firm age, above and beyond the firm and period fixed effects.\nTo test this let’s create a reasonable simulation that approximates the types of regressions we do in empirical corporate finance work. Assume a panel of \\(N = 20,000\\) unique firms for which we have data on some over the period 1981-2010 (a thirty-year panel). Assume that firms enter at some point during that period (for each firm \\(i\\) I will randomly select a year in the period to be the firm founding year). In addition, not all firms make it to the end of the panel (they die out, or get bought, or reincorporate because they don’t enjoy paying taxes and being good corporate citizens), so their end year is randomly selected as some year after their first year but less than or equal to 2010. Finally, from the 20,000 firms, 4,000 are randomly selected to receive treatment in a randomly selected year between their first and last year in the panel. This is approximately what the panel structure looks like for most papers using some combination of Compustat and CRSP.\nFor the treatment effect structure, let’s assume that it manifests in a trend break. That is, the amount of treatment effect incurred by firm \\(i\\) in period \\(t\\), \\(\\delta_{it}\\), is equivalent to \\(0.2 \\cdot (year - treat\\_year + 1)\\), so the effect grows over time (it’s probably more realistic in these types of simulations to model dynamic treatment effects using some log function that grows over a period and then flatlines, but it doesn’t matter for analytical purposes.)\nFor our first analysis, let’s simulate data to make a panel with the structure described above (code is hidden below):\n\n\nCode\n# simulate data -----------------------------------------------------------\n\n# set seed \nset.seed(74792766)\n\n# Generate data - 20,000 firms are placed in each group. Groups 3 and 4 are \n# treated in 1998, Groups 1 and 2 are untreated\n\nmake_data &lt;- function(...) {\n  \n  # Fixed Effects ------------------------------------------------\n  \n  # get a list of 4,000 units that are randomly treated sometime in the panel \n  treated_units &lt;- sample(1:20000, 4000)\n  \n  # unit fixed effects\n  unit &lt;- tibble(\n    unit = 1:20000, \n    unit_fe = rnorm(20000, 0, 1),\n    treat = if_else(unit %in% treated_units, 1, 0)) %&gt;% \n    # make first and last year per unit, and treat year if treated\n    rowwise() %&gt;% \n    mutate(first_year = sample(seq(1981, 2010), 1),\n           # pull last year as a randomly selected date bw first and 2010\n           last_year = if_else(first_year &lt; 2010, sample(seq(first_year, 2010), 1), \n                               as.integer(2010)),\n           # pull treat year as randomly selected year bw first and last if treated\n           treat_year = if_else(treat == 1,\n                                if_else(first_year != last_year,\n                                        sample(first_year:last_year, 1), as.integer(first_year)),\n                                as.integer(0))) %&gt;% \n    ungroup()\n  \n  # year fixed effects \n  year &lt;- tibble(\n    year = 1981:2010,\n    year_fe = rnorm(30, 0, 1))\n  \n  # make panel\n  crossing(unit, year) %&gt;% \n    arrange(unit, year) %&gt;% \n    # keep only if year between first and last year \n    rowwise() %&gt;% \n    filter(year %&gt;% between(first_year, last_year)) %&gt;% \n    ungroup() %&gt;%\n    # make error term, treat term and log age term\n    mutate(error = rnorm(nrow(.), 0, 1),\n           posttreat = if_else(treat == 1 & year &gt;= treat_year, 1, 0),\n           rel_year = if_else(treat == 1, year - treat_year, as.integer(NA)),\n           tau = if_else(posttreat == 1, .2, 0),\n           firm_age = year - first_year,\n           log_age = log(firm_age + 1)) %&gt;% \n    # make cumulative treatment effects\n    group_by(unit) %&gt;% \n    mutate(cumtau = cumsum(tau)) %&gt;% \n    ungroup() %&gt;% \n    # finally, make dummy variables\n    bind_cols(dummy_cols(tibble(lag = .$rel_year), select_columns = \"lag\", \n                         ignore_na = TRUE, remove_selected_columns = TRUE)) %&gt;%\n    # replace equal to 0 for all lead lag columns\n    mutate_at(vars(starts_with(\"lag_\")), ~replace_na(., 0))\n}\n\n# make data\ndata &lt;- make_data()\n\n\nLet’s assume we’re in a world where log of firm age variable (log_age, which is simply the log of year - first_year) has no effect on either the outcome variable or the treatment variable. Thus the causal relationship of the treatment on the outcome is simply reflected as:\n\n\nCode\ndagify(y ~ t, \n       labels = c(\"y\" = \"Outcome\",\n                  \"t\" = \"Treatment\")) %&gt;% \n  ggdag(text = FALSE, use_labels = \"label\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nI’ll estimate the causal effect of the treatment on the outcome using a regular two-way fixed effects (TWFE) regression of the form:\n\\[y_{it} = \\tag{1} \\alpha_i + \\alpha_t + \\sum_{t = min + 1, t \\neq -1}^{max} \\gamma_i \\cdot D_{it} + \\epsilon\\]\nThis is a normal event study design where we fully saturate in relative time indicators, but leave out two relative periods (following Borusyak & Jaravel (2017)), the most negative relative time indicator and the indicator for the year before treatment (t = -1). We know what the true treatment effect is here = 0.2 times the years since treatment for each post-treatment period. I will plot the true treatment effect, along with the estimates from regression (1) below:\n\n\nCode\ntheme_set(theme_clean() + theme(plot.background = element_blank()))\n\n# first make a plot where log age has no impact on the outcome variable\n# get var names in a vector - need to drop the most negative lag (lag_1) and \nmin &lt;- min(data$rel_year, na.rm = TRUE) + 1\nmax &lt;- max(data$rel_year, na.rm = TRUE)\n\n# make string for rel time indicators \nindics &lt;- paste0(c(paste0(\"`lag_\", seq(min, -2), \"`\"),\n                   paste0(\"lag_\", seq(0, max))),\n                 collapse = \" + \")\n\n# make the formula we want to run\nform &lt;- as.formula(paste0(\"y ~ \", indics, \"| unit + year | 0 | unit\"))\n\n# get true taus to merge in\ntrue_taus &lt;- tibble(\n  time = seq(-10, 10),\n  true_tau = c(rep(0, length(-10:-1)), .2*(0:10 + 1))\n)\n\n# estimate the model and make the plot\ndata %&gt;% \n  # generate the outcome variable\n  mutate(y = unit_fe + year_fe + cumtau + error) %&gt;% \n  # run the model and tidy up\n  do(broom::tidy(felm(form, data = .), conf.int = TRUE)) %&gt;%\n  filter(term != \"firm_age\") %&gt;% \n  # make the relative time variable and keep just what we need\n  mutate(time = c(min:(-2), 0:max)) %&gt;% \n  select(time, estimate, conf.low, conf.high) %&gt;% \n  # bring back in missing indicator for t = -1\n  bind_rows(tibble(\n    time = -1, estimate = 0, conf.low = 0, conf.high = 0\n  )) %&gt;% \n  # keep just -10 to 10\n  filter(time %&gt;% between(-10, 10)) %&gt;% \n  left_join(true_taus) %&gt;% \n  # split the error bands by pre-post\n  mutate(band_groups = case_when(\n    time &lt; -1 ~ \"Pre\",\n    time &gt;= 0 ~ \"Post\",\n    time == -1 ~ \"\"\n  )) %&gt;%\n  # plot it\n  ggplot(aes(x = time, y = estimate)) + \n  geom_line(aes(x = time, y = true_tau, color = \"True Effect\"), size = 1.5, linetype = \"dashed\") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, group = band_groups),\n              color = \"lightgrey\", alpha = 1/4) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = \"Estimated Effect\"), \n                  show.legend = FALSE) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  scale_x_continuous(breaks = seq(-10, 10, by = 2)) + \n  labs(x = \"Relative Time\", y = \"Estimate\") +\n  scale_color_brewer(palette = 'Set1') + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16))\n\n\n\n\n\n\n\n\n\nLife is good, we recover the true treatment effect path. As an important aside, note that we fully saturated the time fixed effects here. What if we do what is more common in the literature and bin all year before t - 10 into a Pre variable and all years after t + 10 into a Post variable?\n\n\nCode\ntheme_set(theme_clean() + theme(plot.background = element_blank()))\n\n# first make a plot where log age has no impact on the outcome variable\n# get var names in a vector - need to drop the most negative lag (lag_1) and \nmin &lt;- min(data$rel_year, na.rm = TRUE) + 1\nmax &lt;- max(data$rel_year, na.rm = TRUE)\n\n# make string for rel time indicators \nindics &lt;- paste0(c(\"Pre\", \"Post\", \n                   paste0(\"`lag_\", seq(-10, -2), \"`\"),\n                   paste0(\"lag_\", seq(0, 10))),\n                 collapse = \" + \")\n\n# make the formula we want to run\nform &lt;- as.formula(paste0(\"y ~ \", indics, \"| unit + year | 0 | unit\"))\n\n# estimate the model and make the plot\ndata %&gt;% \n  # make pre and post variables\n  mutate(Pre = if_else(!is.na(rel_year) & rel_year &lt; 10, 1, 0),\n         Post = if_else(!is.na(rel_year) & rel_year &gt; 10, 1, 0)) %&gt;%\n  # generate the outcome variable\n  mutate(y = unit_fe + year_fe + cumtau + error) %&gt;% \n  # run the model and tidy up\n  do(broom::tidy(felm(form, data = .), conf.int = TRUE)) %&gt;%\n  filter(!(term %in% c(\"firm_age\", \"Pre\", \"Post\"))) %&gt;%\n  # make the relative time variable and keep just what we need\n  mutate(time = c(-10:(-2), 0:10)) %&gt;% \n  select(time, estimate, conf.low, conf.high) %&gt;% \n  # bring back in missing indicator for t = -1\n  bind_rows(tibble(\n    time = -1, estimate = 0, conf.low = 0, conf.high = 0\n  )) %&gt;% \n  # keep just -10 to 10\n  filter(time %&gt;% between(-10, 10)) %&gt;% \n  left_join(true_taus) %&gt;% \n  # split the error bands by pre-post\n  mutate(band_groups = case_when(\n    time &lt; -1 ~ \"Pre\",\n    time &gt;= 0 ~ \"Post\",\n    time == -1 ~ \"\"\n  )) %&gt;%\n  # plot it\n  ggplot(aes(x = time, y = estimate)) + \n  geom_line(aes(x = time, y = true_tau, color = \"True Effect\"), size = 1.5, linetype = \"dashed\") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, group = band_groups),\n              color = \"lightgrey\", alpha = 1/4) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = \"Estimated Effect\"), \n                  show.legend = FALSE) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  scale_x_continuous(breaks = seq(-10, 10, by = 2)) + \n  labs(x = \"Relative Time\", y = \"Estimate\") +\n  scale_color_brewer(palette = 'Set1') + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16))\n\n\n\n\n\n\n\n\n\nWhoops! That’s no good. Try messing around with different forms of this, either only putting in the relative time indicators (thus leaving out both Pre and Post) or dropping one or the other, and you’ll see that things get really wonky. It depends on your DGP but we should all be fully saturating the relative time indicators every time.\nMoving on, what if you were to erroneously control for log age even though it isn’t related to either the outcome or the treatment? We’re still good (this should be obvious, but it’s nice to think through these relations and prove it to yourself).\n\n\nCode\ntheme_set(theme_clean() + theme(plot.background = element_blank()))\n\n# make string for rel time indicators \nindics &lt;- paste0(c(paste0(\"`lag_\", seq(min, -2), \"`\"),\n                   paste0(\"lag_\", seq(0, max))),\n                 collapse = \" + \")\n\n# make the formula we want to run\nform &lt;- as.formula(paste0(\"y ~ \", indics, \" + log_age | unit + year | 0 | unit\"))\n\n# estimate the model and make the plot\ndata %&gt;% \n  # generate the outcome variable\n  mutate(y = unit_fe + year_fe + cumtau + error) %&gt;% \n  # run the model and tidy up\n  do(broom::tidy(felm(form, data = .), conf.int = TRUE)) %&gt;%\n  filter(term != \"log_age\") %&gt;% \n  # make the relative time variable and keep just what we need\n  mutate(time = c(min:(-2), 0:max)) %&gt;% \n  select(time, estimate, conf.low, conf.high) %&gt;% \n  # bring back in missing indicator for t = -1\n  bind_rows(tibble(\n    time = -1, estimate = 0, conf.low = 0, conf.high = 0\n  )) %&gt;% \n  # keep just -10 to 10\n  filter(time %&gt;% between(-10, 10)) %&gt;% \n  left_join(true_taus) %&gt;% \n  # split the error bands by pre-post\n  mutate(band_groups = case_when(\n    time &lt; -1 ~ \"Pre\",\n    time &gt;= 0 ~ \"Post\",\n    time == -1 ~ \"\"\n  )) %&gt;%\n  # plot it\n  ggplot(aes(x = time, y = estimate)) + \n  geom_line(aes(x = time, y = true_tau, color = \"True Effect\"), size = 1.5, linetype = \"dashed\") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, group = band_groups),\n              color = \"lightgrey\", alpha = 1/4) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = \"Estimated Effect\"), \n                  show.legend = FALSE) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  scale_x_continuous(breaks = seq(-10, 10, by = 2)) + \n  labs(x = \"Relative Time\", y = \"Estimate\") +\n  scale_color_brewer(palette = 'Set1') + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16))\n\n\n\n\n\n\n\n\n\nOkay, let’s start to make things more interesting. First, what if our log_age variable is associated with our outcome variable, but not with the treatment. Now I change the DGP to be:\n\\[y_{it} = \\tag{1} \\alpha_i + \\alpha_t + \\sum_{t = min + 1, t \\neq -1}^{max} \\gamma_i \\cdot D_{it}  - 0.85 \\cdot log\\_age + \\epsilon\\]\n\n\nCode\ndagify(y ~ t,\n       y ~ x,\n       labels = c(\"y\" = \"Outcome\",\n                  \"t\" = \"Treatment\",\n                  \"x\" = \"Log of Firm Age\")) %&gt;% \n  ggdag(text = FALSE, use_labels = \"label\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nIf we run the regression controlling and control for log_age, we’re good. (Note, you shouldn’t need to control for log_age here because it isn’t a confounding variable - it only impacts the outcome variable. But, there are weird time dynamics that intersect with our relative period dummies and the omitted variables - essentially firm age is associated with the relative time dummies because time only goes in one direction and so does treatment assignment. Too tired to think through the consequences here, but with this DGP you do need to control for log_age to exactly hit the estimates with TWFE).\n\n\nCode\ntheme_set(theme_clean() + theme(plot.background = element_blank()))\n\n# next, assume that there is an effect on the outcome variable but no effect on the treatment outcome\n# make string for rel time indicators \nindics &lt;- paste0(c(paste0(\"`lag_\", seq(min, -2), \"`\"),\n                   paste0(\"lag_\", seq(0, max))),\n                 collapse = \" + \")\n\nform &lt;- as.formula(paste0(\"y ~ \", indics, \"  + log_age| unit + year | 0 | unit\"))\n\n# estimate the model and make the plot\ndata %&gt;% \n  # generate the outcome variable\n  mutate(y = unit_fe + year_fe + cumtau + -.85*log_age + error) %&gt;% \n  # run the model and tidy up\n  do(broom::tidy(felm(form, data = .), conf.int = TRUE)) %&gt;% \n  # make the relative time variable and keep just what we need\n  filter(term != \"log_age\") %&gt;% \n  mutate(time = c(min:(-2), 0:max)) %&gt;% \n  select(time, estimate, conf.low, conf.high) %&gt;% \n  # bring back in missing indicator for t = -1\n  # bring back in missing indicator for t = -1\n  bind_rows(tibble(\n    time = -1, estimate = 0, conf.low = 0, conf.high = 0\n  )) %&gt;% \n  # keep just -10 to 10\n  filter(time %&gt;% between(-10, 10)) %&gt;% \n  left_join(true_taus) %&gt;% \n  # split the error bands by pre-post\n  mutate(band_groups = case_when(\n    time &lt; -1 ~ \"Pre\",\n    time &gt;= 0 ~ \"Post\",\n    time == -1 ~ \"\"\n  )) %&gt;%\n  # plot it\n  ggplot(aes(x = time, y = estimate)) + \n  geom_line(aes(x = time, y = true_tau, color = \"True Effect\"), size = 1.5, linetype = \"dashed\") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, group = band_groups),\n              color = \"lightgrey\", alpha = 1/4) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = \"Estimated Effect\"), \n                  show.legend = FALSE) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  scale_x_continuous(breaks = seq(-10, 10, by = 2)) + \n  labs(x = \"Relative Time\", y = \"Estimate\") +\n  scale_color_brewer(palette = 'Set1') + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16)) \n\n\n\n\n\n\n\n\n\nFinally, what if log_age is a confounding variable? That is, it impacts both the treatment and outcome variable and needs to be controlled for under causal-inference-dag-circle-drawing dogma:\n\n\nCode\ndagify(y ~ t,\n       y ~ x,\n       t ~ x,\n       labels = c(\"y\" = \"Outcome\",\n                  \"t\" = \"Treatment\",\n                  \"x\" = \"Log of Firm Age\")) %&gt;% \n  ggdag(text = FALSE, use_labels = \"label\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nI implement this somewhat cludgely, but essentially I simulate the data in the same manner as above, but now the treatment assignment is in part determined by firm age. For each firm I get their average age in the panel and use that as an inverse weight when I randomly assign the treatment to firms (probably better ways to do this but it works). Now, not only is the estimate still valid when controlling for log_age but you in fact need to control for it to get unbiased estimates.\n\n\nCode\ntheme_set(theme_clean() + theme(plot.background = element_blank()))\n\n# Now assume that the log age variable is correlated in some way with the treatment assignment decision\n# in particular assume that younger firms are more likely to get targeted \n\n# Generate data - 20,000 firms are placed in each group. Groups 3 and 4 are \n# treated in 1998, Groups 1 and 2 are untreated\n\nmake_data &lt;- function(...) {\n  \n  # Fixed Effects ------------------------------------------------\n  # unit fixed effects\n  unit &lt;- tibble(\n    unit = 1:20000, \n    unit_fe = rnorm(20000, 0, 1)) %&gt;%\n    # make first and last year per unit, and treat year if treated\n    rowwise() %&gt;% \n    mutate(first_year = sample(seq(1981, 2010), 1),\n           # pull last year as a randomly selected date bw first and 2010\n           last_year = if_else(first_year &lt; 2010, sample(seq(first_year, 2010), 1), \n                               as.integer(2010))) %&gt;% \n    ungroup()\n  \n  # year fixed effects \n  year &lt;- tibble(\n    year = 1981:2010,\n    year_fe = rnorm(30, 0, 1))\n  \n  # make panel\n  data &lt;- crossing(unit, year) %&gt;% \n    arrange(unit, year) %&gt;% \n    # keep only if year between first and last year \n    rowwise() %&gt;% \n    filter(year %&gt;% between(first_year, last_year)) %&gt;% \n    ungroup() %&gt;% \n    mutate(firm_age = year - first_year)\n  \n    # make an average age data frame \n  avg_age &lt;- data %&gt;% \n    group_by(unit) %&gt;% \n    summarize(avg_age = mean(firm_age)) %&gt;% \n    mutate(weight = 1 / (avg_age + 1))\n  \n  # sample 4,000 firms to get treatment, weighted by average age\n  treated_units &lt;- sample_n(avg_age, 4000, replace = FALSE, weight = weight) %&gt;%\n    mutate(treat = 1) %&gt;% \n    select(unit, treat)\n  \n  # merge treat back into the data \n  treat_data &lt;- data %&gt;% \n    select(unit, first_year, last_year) %&gt;% \n    distinct() %&gt;% \n    left_join(treated_units) %&gt;% \n    replace_na(list(treat = 0)) %&gt;% \n    rowwise() %&gt;% \n    mutate(treat_year = if_else(treat == 1,\n                                if_else(first_year != last_year,\n                                        sample(first_year:last_year, 1), as.integer(first_year)),\n                                as.integer(0)))\n  \n  # merge back into main data\n  data &lt;- data %&gt;% \n    left_join(treat_data) %&gt;% \n    # make error term, treat term and log age term\n    mutate(error = rnorm(nrow(.), 0, 1),\n           posttreat = if_else(treat == 1 & year &gt;= treat_year, 1, 0),\n           rel_year = if_else(treat == 1, year - treat_year, as.integer(NA)),\n           tau = if_else(posttreat == 1, .2, 0),\n           firm_age = year - first_year,\n           log_age = log(firm_age + 1)) %&gt;% \n    # make cumulative treatment effects\n    group_by(unit) %&gt;% \n    mutate(cumtau = cumsum(tau)) %&gt;% \n    ungroup() %&gt;% \n    # finally, make dummy variables\n    bind_cols(dummy_cols(tibble(lag = .$rel_year), select_columns = \"lag\", \n                         ignore_na = TRUE, remove_selected_columns = TRUE)) %&gt;%\n    # replace equal to 0 for all lead lag columns\n    mutate_at(vars(starts_with(\"lag_\")), ~replace_na(., 0))\n}\n\n# make data\ndata2 &lt;- make_data()\n\n# get var names in a vector - need to drop the most negative lag (lag_1) and \nmin &lt;- min(data2$rel_year, na.rm = TRUE) + 1\nmax &lt;- max(data2$rel_year, na.rm = TRUE)\n\n# make string for rel time indicators \nindics &lt;- paste0(c(paste0(\"`lag_\", seq(min, -2), \"`\"),\n                   paste0(\"lag_\", seq(0, max))),\n                 collapse = \" + \")\n\n# make the formula we want to run\nform1 &lt;- as.formula(paste0(\"y ~ \", indics, \"| unit + year | 0 | unit\"))\nform2 &lt;- as.formula(paste0(\"y ~ \", indics, \" + log_age| unit + year | 0 | unit\"))\n\n# estimate the model and make the plot\ndata2 %&gt;% \n  # generate the outcome variable\n  mutate(y = unit_fe + year_fe + cumtau + -.85*log_age + error) %&gt;% \n  # run the model and tidy up\n  do(bind_rows(\n    broom::tidy(felm(form1, data = .), conf.int = TRUE) %&gt;% mutate(mod = \"No Control Log Age\"),\n    broom::tidy(felm(form2, data = .), conf.int = TRUE) %&gt;% mutate(mod = \"Control Log Age\"))) %&gt;% \n  # make the relative time variable and keep just what we need\n  filter(term != \"log_age\") %&gt;% \n  mutate(time = rep(c(min:(-2), 0:max), 2)) %&gt;% \n  select(time, estimate, conf.low, conf.high, mod) %&gt;% \n  # bring back in missing indicator for t = -1\n  bind_rows(tibble(\n    time = rep(-1, 2), estimate = rep(0, 2), conf.low = rep(0, 2),\n    conf.high = rep(0, 2), mod = c(\"No Control Log Age\", \"Control Log Age\")\n  )) %&gt;% \n  # keep just -10 to 10\n  filter(time %&gt;% between(-10, 10)) %&gt;% \n  left_join(true_taus) %&gt;% \n  # split the error bands by pre-post\n  mutate(band_groups = case_when(\n    time &lt; -1 ~ \"Pre\",\n    time &gt;= 0 ~ \"Post\",\n    time == -1 ~ \"\"\n  )) %&gt;%\n  # plot it\n  ggplot(aes(x = time, y = estimate)) + \n  geom_line(aes(x = time, y = true_tau, color = \"True Effect\"), size = 1.5, linetype = \"dashed\") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, group = band_groups),\n              color = \"lightgrey\", alpha = 1/4) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = \"Estimated Effect\"), \n                  show.legend = FALSE) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  scale_x_continuous(breaks = seq(-10, 10, by = 2)) + \n  labs(x = \"Relative Time\", y = \"Estimate\") +\n  scale_color_brewer(palette = 'Set1') + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16),\n        panel.spacing = unit(2, \"lines\")) + \n  facet_wrap(~mod)\n\n\n\n\n\n\n\n\n\nIn conclusion, at least in this very straightforward example, you do need to control for the log of firm age, even though firm age itself is collinear with the fixed effects structure. Mea culpa."
  },
  {
    "objectID": "research/articles/slr/index.html",
    "href": "research/articles/slr/index.html",
    "title": "Single-Firm Event Studies, Securities Fraud, and Financial Crisis: Problems of Inference",
    "section": "",
    "text": "Paper\nCode\nData"
  },
  {
    "objectID": "research/articles/slr/index.html#links",
    "href": "research/articles/slr/index.html#links",
    "title": "Single-Firm Event Studies, Securities Fraud, and Financial Crisis: Problems of Inference",
    "section": "",
    "text": "Paper\nCode\nData"
  },
  {
    "objectID": "research/articles/slr/index.html#abstract",
    "href": "research/articles/slr/index.html#abstract",
    "title": "Single-Firm Event Studies, Securities Fraud, and Financial Crisis: Problems of Inference",
    "section": "Abstract",
    "text": "Abstract\nLawsuits brought pursuant to section 10(b) of the Securities and Exchange Act depend on the reliability of a statistical tool called an event study to adjudicate issues of reliance, materiality, loss causation, and damages. Although judicial acceptance of the event study technique is pervasive, there has been little empirical analysis of the ability of event studies to produce reliable results when applied to a single company’s security.\nUsing data from the recent financial crisis, this Note demonstrates that the standard-model event study used in most court proceedings can lead to biased inferences sanctioned through the Daubert standard of admissibility for expert testimony. In particular, in the presence of broad market volatility, a base event study will cause too many returns to be identified as statistically significant. Even recently proposed variations of the event study model specifically designed to address violations of the statistical assumptions of an event study will not completely correct this bias.\nThis Note proposes two alternative forms of event studies that are capable of creating statistically reliable results and should be adopted by courts in instances where there is cause to believe that market volatility has increased. Over previous decades, the judiciary has steadily moved toward a reliance on empirics and expert testimony in overseeing complex civil cases. Yet there has been surprisingly little research accompanying this judicial deference on the ability of statistical evidence to produce the promised result. This Note calls into question whether this movement has been beneficial from a logical or empirical perspective, but it demonstrates that alternative techniques that can aid the finder of fact in resolving these disputes—regardless of market trends—may in fact exist."
  },
  {
    "objectID": "research/articles/slr/index.html#important-figure",
    "href": "research/articles/slr/index.html#important-figure",
    "title": "Single-Firm Event Studies, Securities Fraud, and Financial Crisis: Problems of Inference",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFigure 3\n\n\n@article{baker2016single,\n  title={Single-firm event studies, securities fraud, and financial crisis: problems of inference},\n  author={Baker, Andrew C},\n  journal={Stan. L. Rev.},\n  volume={68},\n  pages={1207},\n  year={2016},\n  publisher={HeinOnline}\n}"
  },
  {
    "objectID": "research/articles/diversity_washing/index.html",
    "href": "research/articles/diversity_washing/index.html",
    "title": "Diversity Washing",
    "section": "",
    "text": "Paper"
  },
  {
    "objectID": "research/articles/diversity_washing/index.html#paper",
    "href": "research/articles/diversity_washing/index.html#paper",
    "title": "Diversity Washing",
    "section": "",
    "text": "Paper"
  },
  {
    "objectID": "research/articles/diversity_washing/index.html#abstract",
    "href": "research/articles/diversity_washing/index.html#abstract",
    "title": "Diversity Washing",
    "section": "Abstract",
    "text": "Abstract\nWe provide large-sample evidence on whether U.S. publicly traded corporations use voluntary disclosures about their commitments to employee diversity opportunistically. We document significant discrepancies between companies’ external stances on diversity, equity, and inclusion (DEI) and their hiring practices. Firms that discuss DEI excessively relative to their actual employee gender and racial diversity (“diversity washers”) obtain superior scores from environmental, social, and governance (ESG) rating organizations and attract more investment from institutional investors with an ESG focus. These outcomes occur even though diversity-washing firms are more likely to incur discrimination violations and have negative human-capital-related news events. Our study provides evidence consistent with growing allegations of misleading statements from firms about their DEI initiatives and highlights the potential consequences of selective ESG disclosures."
  },
  {
    "objectID": "research/articles/diversity_washing/index.html#important-figure",
    "href": "research/articles/diversity_washing/index.html#important-figure",
    "title": "Diversity Washing",
    "section": "Important figure",
    "text": "Important figure\nFigure 5: Nonparametric summary of ESG outcomes by DEI discussion and diversity ranks. This figure presents the average of Refinitiv’s Overall Score (panel A), Refinitiv’s Social Score (panel B), Sustainalytics Overall Score (panel C), Sustainalytics Social Score (panel D), Ownership by US SIF funds (panel E), and Ownership by ESG funds, based on the fund name (panel F). Each heat map is broken down by the decile of DEI discussion (measured by the overall amount of DEI discussion in our corpus of SEC documents) and the decile of underlying diversity (measured by the percentage of a firm’s U.S.-based workforce that is either female or non-white)\n\n\n\nFigure 5\n\n\n@article{baker2022diversity,\n  title={Diversity washing},\n  author={Baker, Andrew C and Larcker, David F and McCLURE, CHARLES G and Saraph, Durgesh and Watts, Edward M},\n  journal={Journal of Accounting Research},\n  year={2022},\n  publisher={Wiley Online Library}\n}"
  },
  {
    "objectID": "research/articles/bagel/index.html",
    "href": "research/articles/bagel/index.html",
    "title": "Machine Learning and Predicted Returns for Event Studies in Securities Litigation",
    "section": "",
    "text": "Paper\nCode"
  },
  {
    "objectID": "research/articles/bagel/index.html#links",
    "href": "research/articles/bagel/index.html#links",
    "title": "Machine Learning and Predicted Returns for Event Studies in Securities Litigation",
    "section": "",
    "text": "Paper\nCode"
  },
  {
    "objectID": "research/articles/bagel/index.html#abstract",
    "href": "research/articles/bagel/index.html#abstract",
    "title": "Machine Learning and Predicted Returns for Event Studies in Securities Litigation",
    "section": "Abstract",
    "text": "Abstract\nWe investigate the use of machine learning (ML) and other robust estimation techniques in event studies conducted on single securities for the purpose of securities litigation. Single-firm event studies are widely used in civil litigation, with billions of dollars in settlements hinging on the outcome of the exercise. We find that regularization (equivalently, penalized estimation) can yield noticeable improvements in both the variance of event-date abnormal returns and significance-test power. Thus we believe that there is a role for ML methods in event studies used in securities litigation. At the same time, we find that ML-induced performance improvements are smaller than those based on other good practices. Most important are (i) the use of a peer index based on returns for firms in similar industries (how this is computed appears to be less important than that some version be included), and (ii) for significance testing, using the SQ test proposed in Gelbach et al. (2013), because it is robust to the considerable non-normality present in abnormal returns."
  },
  {
    "objectID": "research/articles/bagel/index.html#important-figure",
    "href": "research/articles/bagel/index.html#important-figure",
    "title": "Machine Learning and Predicted Returns for Event Studies in Securities Litigation",
    "section": "Important figure",
    "text": "Important figure\nFigure 1 plots specification ranks. Each specification has 8 MSE performance values: two time periods (1999–2009 vs. 2009–2019), with and without the FFC factors, and two MSE normalization approaches (\\(\\widehat{R}_{oos}^{k}\\) and \\(\\widehat{R}_{het}^k\\), described below). Each gray dot represents a rank from 1 to 11, and each rank is represented once for each of the eight time-period/FFC-factor/MSE metric combinations. The diamonds plot the specifications’ average ranks. Blue diamonds signify models that allow firms to enter the regression function individually and use cross-validation and penalized regression; red diamonds represent specifications that do not.\n\n\n\nFigure 1\n\n\n@article{baker2020machine,\n  title={Machine learning and predicted returns for event studies in securities litigation},\n  author={Baker, Andrew and Gelbach, Jonah B and others},\n  journal={Journal of Law, Finance, and Accounting},\n  volume={5},\n  number={2},\n  pages={231--272},\n  year={2020},\n  publisher={Now Publishers, Inc.}\n}"
  },
  {
    "objectID": "research/working_papers/antitakeover/index.html",
    "href": "research/working_papers/antitakeover/index.html",
    "title": "State Antitakeover Provisions Don’t Actually Do Much",
    "section": "",
    "text": "Paper\nSSRN\nCode"
  },
  {
    "objectID": "research/working_papers/antitakeover/index.html#links",
    "href": "research/working_papers/antitakeover/index.html#links",
    "title": "State Antitakeover Provisions Don’t Actually Do Much",
    "section": "",
    "text": "Paper\nSSRN\nCode"
  },
  {
    "objectID": "research/working_papers/antitakeover/index.html#abstract",
    "href": "research/working_papers/antitakeover/index.html#abstract",
    "title": "State Antitakeover Provisions Don’t Actually Do Much",
    "section": "Abstract",
    "text": "Abstract\nCorporate governance scholars have engaged in a longstanding debate over the impact of state antitakeover provisions. Corporate law practitioners and researchers argue that the affirmation of the “poison pill” made the second generation of antitakeover statutes redundant, while empirical scholars in corporate finance continue to find wide-ranging impacts from their adoption. This paper subjects the standard approach used in the empirical literature to a series of straightforward extensions using current best practice in panel data analysis. Contrary to the majority of published research, there is scant evidence for a consistent and reliable impact of antitakeover statute adoption on firm activity. These findings follow logically from the legal argument that takeover statutes provide little additional takeover deterrence in the presence of a “shadow pill.”"
  },
  {
    "objectID": "research/working_papers/antitakeover/index.html#important-figure",
    "href": "research/working_papers/antitakeover/index.html#important-figure",
    "title": "State Antitakeover Provisions Don’t Actually Do Much",
    "section": "Important figure",
    "text": "Important figure\nFigure 10 reports the event study estimates of the impact of poison pill law changes using the estimator from Callaway and Sant’Anna (2020) and the data and design changes described in Section 5.1. Model 1 includes only the fixed effects without any covariates, Model 2 includes the covariates in the short regression model from Karpoff and Wittry (2018), and Model 3 includes their full model.\n\n\n\nFigure 10"
  },
  {
    "objectID": "research/working_papers/activism/index.html",
    "href": "research/working_papers/activism/index.html",
    "title": "The Effects of Hedge Fund Activism",
    "section": "",
    "text": "Paper"
  },
  {
    "objectID": "research/working_papers/activism/index.html#links",
    "href": "research/working_papers/activism/index.html#links",
    "title": "The Effects of Hedge Fund Activism",
    "section": "",
    "text": "Paper"
  },
  {
    "objectID": "research/working_papers/activism/index.html#abstract",
    "href": "research/working_papers/activism/index.html#abstract",
    "title": "The Effects of Hedge Fund Activism",
    "section": "Abstract",
    "text": "Abstract\nIn this paper I explore the relationship between the rise of hedge fund activism and firm outcomes, using a study design that explicitly takes into account how activists pick their targets. Contrary to much prior work, I find no evidence that activism is associated with increased firm operating performance or significant long-term returns once comparing to firms based on their similarity to the targets. However, activism does increase firm payouts to shareholders and decreases investment, consistent with the argument of many critics of activism. I also find that firm-level employment declines significantly following a targeting event, and that the subset of firms that experience an increase in operating performance also engage in higher levels of tax avoidance. The deregulation of proxy access rules, wholesale de-staggering of corporate boards, and the rise in importance of proxy advisory firms who frequently recommend voting for activist proposals have made firms more susceptible to aggressive activism over the past three decades. The results in this paper, coupled with the rhetorical shift in focus from short-term profits to sustainable growth by large institutional investors, suggest a re-framing of the public debate over the benefits of shareholder activism."
  },
  {
    "objectID": "research/working_papers/activism/index.html#important-figure",
    "href": "research/working_papers/activism/index.html#important-figure",
    "title": "The Effects of Hedge Fund Activism",
    "section": "Important figure",
    "text": "Important figure\nFigure 11 reports the average long-term buy-and-hold abnormal returns for the targeted  rms in our sample with its 95% confidence interval (in blue), as well as the average of the propensity-score weighted comparison returns for each target (in red). Panel (a) presents the results for  rms that are taken over within the five years following the event or pseudo-event, and Panel (b) reports the results for firms that are not taken over during this period.\n\n\n\nFigure 1"
  },
  {
    "objectID": "research/working_papers/ml_comp/index.html",
    "href": "research/working_papers/ml_comp/index.html",
    "title": "Validating Valuation: How Statistical Learning Can Cabin Expert Discretion in Valuation Disputes",
    "section": "",
    "text": "Paper\nSSRN"
  },
  {
    "objectID": "research/working_papers/ml_comp/index.html#links",
    "href": "research/working_papers/ml_comp/index.html#links",
    "title": "Validating Valuation: How Statistical Learning Can Cabin Expert Discretion in Valuation Disputes",
    "section": "",
    "text": "Paper\nSSRN"
  },
  {
    "objectID": "research/working_papers/ml_comp/index.html#abstract",
    "href": "research/working_papers/ml_comp/index.html#abstract",
    "title": "Validating Valuation: How Statistical Learning Can Cabin Expert Discretion in Valuation Disputes",
    "section": "Abstract",
    "text": "Abstract\nThis paper challenges conventional methods used in financial valuation across areas in litigation practice. We use a large-scale empirical simulation, using real firm data, to demonstrate that the widely used “comparable companies” approach allows enormous expert discretion, which enables substantial inconsistency and subjective judgment in court valuations. We then use the same simulation approach to show that using better data choices together with contemporary penalized regression methods generates valuation estimates that are considerably less variable, thereby reducing the scope for expert bias. We also apply this approach to a recent Delaware valuation dispute. This paper should transform financial valuation practice in litigation by both diagnosing and offering a cure for excessive discretion and variability in valuation disputes. Our methods would lead to better performing and more empirically grounded outcomes in legal disputes involving valuation, thus enhancing the fairness and efficiency of the judicial processes in valuation litigation."
  },
  {
    "objectID": "research/working_papers/ml_comp/index.html#important-figure",
    "href": "research/working_papers/ml_comp/index.html#important-figure",
    "title": "Validating Valuation: How Statistical Learning Can Cabin Expert Discretion in Valuation Disputes",
    "section": "Important figure",
    "text": "Important figure\nThis figure reports the valuation estimates for Landstar \\(d^*\\) = 43 trading days after the end of 2017 Q4 using different input combinations. The top panel plots the estimated market value from each of the 24 combinations, which are represented by the grey tiles in the lower panel. The combinations vary based on the choice of matching feature set, number of matches, distance measure, and summary measure.\n\n\n\nFigure 1"
  },
  {
    "objectID": "research/articles/dual_class/index.html",
    "href": "research/articles/dual_class/index.html",
    "title": "Dual-Class Index Exclusion",
    "section": "",
    "text": "Paper"
  },
  {
    "objectID": "research/articles/dual_class/index.html#links",
    "href": "research/articles/dual_class/index.html#links",
    "title": "Dual-Class Index Exclusion",
    "section": "",
    "text": "Paper"
  },
  {
    "objectID": "research/articles/dual_class/index.html#abstract",
    "href": "research/articles/dual_class/index.html#abstract",
    "title": "Dual-Class Index Exclusion",
    "section": "Abstract",
    "text": "Abstract\nOne of the most contentious and long-standing debates in corporate governance is whether company founders and other insiders should be permitted to use multi-class stock structures with unequal votes to control their companies while seeking capital through a public listing. Stymied by the permissive attitudes of legislatures and regulators, institutional investors opposed to multi-class arrangements recently turned to a new potential source of regulation: benchmark equity index providers. At the behest of institutional investors, the three largest index providers recently changed the eligibility requirements for their benchmark equity indexes to exclude, limit or underweight companies with multi-class stock structures. Investors expected the prospect of exclusion from such indexes to discourage founders and directors from adopting dual-class stock structures in connection with their initial public offerings.\nWhile there is a voluminous financial literature on the effects of index inclusion and exclusion on stock prices, and legal scholars have recently explored the corporate governance implications of the exponential growth of passive index investing, focusing primarily on the incentives of index fund asset managers, neither the financial nor the legal literature have considered the corporate governance role and influence of the parties who write the rules for index investing: the index providers. We begin to fill this gap in the literature by assessing the efficacy of index providers as corporate governance arbiters through the rubric of their dual-class index exclusion decisions.\nWe start with the premise that the index exclusion sanction will not discourage dual-class listings unless it is sufficiently costly to outweigh the perceived benefits of founder control through a multi-class stock structure. We expect the index exclusion sanction will not be sufficiently costly for several reasons. First, it is difficult, if not impossible, to implement a sanction through the public capital markets. Second, the index inclusion effect on which the anticipated sanction is premised has effectively disappeared in recent years and may never have been a long-term source of lower capital costs. Third, despite the explosive growth of index investing in recent years, funds following stock indexes still hold a relatively modest percentage of the market capitalization of U.S. equities – around 12% according to BlackRock. Finally, the proliferation of index investing opportunities has weakened the market-moving influence of any one benchmark index.\nTo test the efficacy of the sanction, we conduct an event study of the S&P announcement that dual-class companies would henceforth be excluded from the S&P 1500 Composite Index and its components – the S&P 500, S&P 400 mid-cap and S&P 600 small-cap indices. Because S&P grandfathered dual-class companies currently in the index, we are able to compare movements in the stock prices of dual-class companies currently in the index with movements in the stock prices of dual-class companies not yet included in the index at the time of announcement. We do not observe any statistically significant abnormal returns in the stock prices of either included or excluded firms as a result of the S&P announcement, suggesting that exclusion is not expected to have a significant adverse cost of capital effect on firms that elect to list with a dual-class stock structures in the future and the sanction is ineffective. In the absence of an effective sanction, the exclusion of dual-class shares from benchmark equity indexes will not affect corporate governance choices. It may, however, have material adverse consequences for index investors and the index providers themselves."
  },
  {
    "objectID": "research/articles/dual_class/index.html#important-figure",
    "href": "research/articles/dual_class/index.html#important-figure",
    "title": "Dual-Class Index Exclusion",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFigure 1\n\n\n@article{winden2019dual,\n  title={Dual-Class Index Exclusion},\n  author={Winden, Andrew and Baker, Andrew},\n  journal={Va. L. \\& Bus. Rev.},\n  volume={13},\n  pages={101},\n  year={2019},\n  publisher={HeinOnline}\n}"
  },
  {
    "objectID": "research/articles/staggered_did/index.html",
    "href": "research/articles/staggered_did/index.html",
    "title": "How Much Should We Trust Staggered Difference-in-Differences Estimates?",
    "section": "",
    "text": "Paper\nCode"
  },
  {
    "objectID": "research/articles/staggered_did/index.html#links",
    "href": "research/articles/staggered_did/index.html#links",
    "title": "How Much Should We Trust Staggered Difference-in-Differences Estimates?",
    "section": "",
    "text": "Paper\nCode"
  },
  {
    "objectID": "research/articles/staggered_did/index.html#abstract",
    "href": "research/articles/staggered_did/index.html#abstract",
    "title": "How Much Should We Trust Staggered Difference-in-Differences Estimates?",
    "section": "Abstract",
    "text": "Abstract\nWe explain when and how staggered difference-in-differences regression estimators, commonly applied to assess the impact of policy changes, are biased. These biases are likely to be relevant for a large portion of research settings in finance, accounting, and law that rely on staggered treatment timing, and can result in Type-I and Type-II errors. We summarize three alternative estimators developed in the econometrics and applied literature for addressing these biases, including their differences and tradeoffs. We apply these estimators to re-examine prior published results and show, in many cases, the alternative causal estimates or inferences differ substantially from prior papers."
  },
  {
    "objectID": "research/articles/staggered_did/index.html#important-figure",
    "href": "research/articles/staggered_did/index.html#important-figure",
    "title": "How Much Should We Trust Staggered Difference-in-Differences Estimates?",
    "section": "Important figure",
    "text": "Important figure\nThe top panel of Fig. 3 illustrates the diagnostic test for Simulations 4, 5, and 6. Because the diagnostic test only applies to balanced panels, in constructing this figure our simulation is modified to artificially induce a balanced panel of firm-year observations from Compustat before drawing fixed effects and residuals from the empirical distribution.\n\n\n\nFigure 3\n\n\n@article{baker2022much,\n  title={How much should we trust staggered difference-in-differences estimates?},\n  author={Baker, Andrew C and Larcker, David F and Wang, Charles CY},\n  journal={Journal of Financial Economics},\n  volume={144},\n  number={2},\n  pages={370--395},\n  year={2022},\n  publisher={Elsevier}\n}"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "Working Paper\n\n\n\n\n\n2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking Paper\n\n\n\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking Paper\n\n\n\n\n\n2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#working-papers",
    "href": "research/index.html#working-papers",
    "title": "Research",
    "section": "",
    "text": "Working Paper\n\n\n\n\n\n2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking Paper\n\n\n\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking Paper\n\n\n\n\n\n2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#published-papers",
    "href": "research/index.html#published-papers",
    "title": "Research",
    "section": "Published Papers",
    "text": "Published Papers\n\n\n\n\n\n\n\n\n\n\nDiversity Washing\n\n\n\n\n\nJournal of Accounting Research 2024 (Joint with David F. Larcker, Charles G. McClure, Durgesh Saraph, and Edward M. Watts)\n\n\n\n\n\n2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow Much Should We Trust Staggered Difference-in-Differences Estimates?\n\n\n\n\n\nJournal of Financial Economics 22(2) 2022 (Joint with David F. Larcker and Charles C.Y. Wang)\n\n\n\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning and Predicted Returns for Event Studies in Securities Litigation\n\n\n\n\n\nJournal of Law, Finance, and Accounting 5(2) 2020 (Joint with Jonah B. Gelbach)\n\n\n\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\n\nDual-Class Index Exclusion\n\n\n\n\n\nVirginia Law & Business Review  13(2) 2019 (Joint with Andrew Winden)\n\n\n\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\n\nSingle-Firm Event Studies, Securities Fraud, and Financial Crisis: Problems of Inference\n\n\n\n\n\nStanford Law Review 68 2016\n\n\n\n\n\n2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/posts/log_age/index.html",
    "href": "posts/posts/log_age/index.html",
    "title": "Controlling for Log Age",
    "section": "",
    "text": "In doing research for my dissertation I keep on running across models that control for somewhat arbitrary variables (or where at least there is no justification for their inclusion). This is common in applied corporate finance / managerial accounting papers. As a result, I snarked.\n\n\nThe issue here is that in regressions for some outcome - say firm valuation (the dreaded Q) - we want to look at the the change in the outcome variable around some treatment shock, but we want to control for some variables. The big ones here are firm age and firm size, using as a proxy firm market value (yes I realize it makes little sense to regress the firm valuation ratio — Tobin’s Q \\(\\approx\\) market value / book value — on the market value but it’s what people do).\nA problem arises when you use firm fixed effects and period fixed effects, because firm age becomes collinear with the fixed effect structure. As a result researchers will often control for the log of firm value instead, which is not perfectly collinear. This strikes me as very odd, because firm age is still an identity that comes directly from a firm variable (the first year in the panel) and a fixed effect (the period FE). So, I figured I would simulate some data to see if it works as we might hope it does.\nAssume we’re in a setting where the difference-in-differences (DiD) issues that I’ve written about previously here don’t apply - namely that there are many more non-treated units than treated units so that the negative weighting issue identified by Andrew Goodman-Bacon and others does not bite. Assume that we think the data generating process is as follows:\n\\[ y_{it} = \\alpha_i + \\alpha_t + \\delta T_{it} + \\epsilon_{it} \\\\\n\\alpha_i \\sim N(0, 1) \\\\\n\\alpha_t \\sim N(0, 1) \\\\\n\\epsilon_{it} \\sim N(0, 1) \\]\nThat is, our outcome variable for firm \\(i\\) in period \\(t\\) is a linear function of a firm specific effect \\(\\alpha_i\\), a period effect \\(\\alpha_t\\), a treatment effect \\(\\delta\\) and a stochastic error term \\(\\epsilon_{it}\\). Assume that, for some reason, we want to control for firm age, above and beyond the firm and period fixed effects.\nTo test this let’s create a reasonable simulation that approximates the types of regressions we do in empirical corporate finance work. Assume a panel of \\(N = 20,000\\) unique firms for which we have data on some over the period 1981-2010 (a thirty-year panel). Assume that firms enter at some point during that period (for each firm \\(i\\) I will randomly select a year in the period to be the firm founding year). In addition, not all firms make it to the end of the panel (they die out, or get bought, or reincorporate because they don’t enjoy paying taxes and being good corporate citizens), so their end year is randomly selected as some year after their first year but less than or equal to 2010. Finally, from the 20,000 firms, 4,000 are randomly selected to receive treatment in a randomly selected year between their first and last year in the panel. This is approximately what the panel structure looks like for most papers using some combination of Compustat and CRSP.\nFor the treatment effect structure, let’s assume that it manifests in a trend break. That is, the amount of treatment effect incurred by firm \\(i\\) in period \\(t\\), \\(\\delta_{it}\\), is equivalent to \\(0.2 \\cdot (year - treat\\_year + 1)\\), so the effect grows over time (it’s probably more realistic in these types of simulations to model dynamic treatment effects using some log function that grows over a period and then flatlines, but it doesn’t matter for analytical purposes.)\nFor our first analysis, let’s simulate data to make a panel with the structure described above (code is hidden below):\n\n\nCode\n# simulate data -----------------------------------------------------------\n\n# set seed \nset.seed(74792766)\n\n# Generate data - 20,000 firms are placed in each group. Groups 3 and 4 are \n# treated in 1998, Groups 1 and 2 are untreated\n\nmake_data &lt;- function(...) {\n  \n  # Fixed Effects ------------------------------------------------\n  \n  # get a list of 4,000 units that are randomly treated sometime in the panel \n  treated_units &lt;- sample(1:20000, 4000)\n  \n  # unit fixed effects\n  unit &lt;- tibble(\n    unit = 1:20000, \n    unit_fe = rnorm(20000, 0, 1),\n    treat = if_else(unit %in% treated_units, 1, 0)) %&gt;% \n    # make first and last year per unit, and treat year if treated\n    rowwise() %&gt;% \n    mutate(first_year = sample(seq(1981, 2010), 1),\n           # pull last year as a randomly selected date bw first and 2010\n           last_year = if_else(first_year &lt; 2010, sample(seq(first_year, 2010), 1), \n                               as.integer(2010)),\n           # pull treat year as randomly selected year bw first and last if treated\n           treat_year = if_else(treat == 1,\n                                if_else(first_year != last_year,\n                                        sample(first_year:last_year, 1), as.integer(first_year)),\n                                as.integer(0))) %&gt;% \n    ungroup()\n  \n  # year fixed effects \n  year &lt;- tibble(\n    year = 1981:2010,\n    year_fe = rnorm(30, 0, 1))\n  \n  # make panel\n  crossing(unit, year) %&gt;% \n    arrange(unit, year) %&gt;% \n    # keep only if year between first and last year \n    rowwise() %&gt;% \n    filter(year %&gt;% between(first_year, last_year)) %&gt;% \n    ungroup() %&gt;%\n    # make error term, treat term and log age term\n    mutate(error = rnorm(nrow(.), 0, 1),\n           posttreat = if_else(treat == 1 & year &gt;= treat_year, 1, 0),\n           rel_year = if_else(treat == 1, year - treat_year, as.integer(NA)),\n           tau = if_else(posttreat == 1, .2, 0),\n           firm_age = year - first_year,\n           log_age = log(firm_age + 1)) %&gt;% \n    # make cumulative treatment effects\n    group_by(unit) %&gt;% \n    mutate(cumtau = cumsum(tau)) %&gt;% \n    ungroup() %&gt;% \n    # finally, make dummy variables\n    bind_cols(dummy_cols(tibble(lag = .$rel_year), select_columns = \"lag\", \n                         ignore_na = TRUE, remove_selected_columns = TRUE)) %&gt;%\n    # replace equal to 0 for all lead lag columns\n    mutate_at(vars(starts_with(\"lag_\")), ~replace_na(., 0))\n}\n\n# make data\ndata &lt;- make_data()\n\n\nLet’s assume we’re in a world where log of firm age variable (log_age, which is simply the log of year - first_year) has no effect on either the outcome variable or the treatment variable. Thus the causal relationship of the treatment on the outcome is simply reflected as:\n\n\nCode\ndagify(y ~ t, \n       labels = c(\"y\" = \"Outcome\",\n                  \"t\" = \"Treatment\")) %&gt;% \n  ggdag(text = FALSE, use_labels = \"label\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nI’ll estimate the causal effect of the treatment on the outcome using a regular two-way fixed effects (TWFE) regression of the form:\n\\[y_{it} = \\tag{1} \\alpha_i + \\alpha_t + \\sum_{t = min + 1, t \\neq -1}^{max} \\gamma_i \\cdot D_{it} + \\epsilon\\]\nThis is a normal event study design where we fully saturate in relative time indicators, but leave out two relative periods (following Borusyak & Jaravel (2017)), the most negative relative time indicator and the indicator for the year before treatment (t = -1). We know what the true treatment effect is here = 0.2 times the years since treatment for each post-treatment period. I will plot the true treatment effect, along with the estimates from regression (1) below:\n\n\nCode\ntheme_set(theme_clean() + theme(plot.background = element_blank()))\n\n# first make a plot where log age has no impact on the outcome variable\n# get var names in a vector - need to drop the most negative lag (lag_1) and \nmin &lt;- min(data$rel_year, na.rm = TRUE) + 1\nmax &lt;- max(data$rel_year, na.rm = TRUE)\n\n# make string for rel time indicators \nindics &lt;- paste0(c(paste0(\"`lag_\", seq(min, -2), \"`\"),\n                   paste0(\"lag_\", seq(0, max))),\n                 collapse = \" + \")\n\n# make the formula we want to run\nform &lt;- as.formula(paste0(\"y ~ \", indics, \"| unit + year | 0 | unit\"))\n\n# get true taus to merge in\ntrue_taus &lt;- tibble(\n  time = seq(-10, 10),\n  true_tau = c(rep(0, length(-10:-1)), .2*(0:10 + 1))\n)\n\n# estimate the model and make the plot\ndata %&gt;% \n  # generate the outcome variable\n  mutate(y = unit_fe + year_fe + cumtau + error) %&gt;% \n  # run the model and tidy up\n  do(broom::tidy(felm(form, data = .), conf.int = TRUE)) %&gt;%\n  filter(term != \"firm_age\") %&gt;% \n  # make the relative time variable and keep just what we need\n  mutate(time = c(min:(-2), 0:max)) %&gt;% \n  select(time, estimate, conf.low, conf.high) %&gt;% \n  # bring back in missing indicator for t = -1\n  bind_rows(tibble(\n    time = -1, estimate = 0, conf.low = 0, conf.high = 0\n  )) %&gt;% \n  # keep just -10 to 10\n  filter(time %&gt;% between(-10, 10)) %&gt;% \n  left_join(true_taus) %&gt;% \n  # split the error bands by pre-post\n  mutate(band_groups = case_when(\n    time &lt; -1 ~ \"Pre\",\n    time &gt;= 0 ~ \"Post\",\n    time == -1 ~ \"\"\n  )) %&gt;%\n  # plot it\n  ggplot(aes(x = time, y = estimate)) + \n  geom_line(aes(x = time, y = true_tau, color = \"True Effect\"), size = 1.5, linetype = \"dashed\") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, group = band_groups),\n              color = \"lightgrey\", alpha = 1/4) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = \"Estimated Effect\"), \n                  show.legend = FALSE) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  scale_x_continuous(breaks = seq(-10, 10, by = 2)) + \n  labs(x = \"Relative Time\", y = \"Estimate\") +\n  scale_color_brewer(palette = 'Set1') + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16))\n\n\n\n\n\n\n\n\n\nLife is good, we recover the true treatment effect path. As an important aside, note that we fully saturated the time fixed effects here. What if we do what is more common in the literature and bin all year before t - 10 into a Pre variable and all years after t + 10 into a Post variable?\n\n\nCode\ntheme_set(theme_clean() + theme(plot.background = element_blank()))\n\n# first make a plot where log age has no impact on the outcome variable\n# get var names in a vector - need to drop the most negative lag (lag_1) and \nmin &lt;- min(data$rel_year, na.rm = TRUE) + 1\nmax &lt;- max(data$rel_year, na.rm = TRUE)\n\n# make string for rel time indicators \nindics &lt;- paste0(c(\"Pre\", \"Post\", \n                   paste0(\"`lag_\", seq(-10, -2), \"`\"),\n                   paste0(\"lag_\", seq(0, 10))),\n                 collapse = \" + \")\n\n# make the formula we want to run\nform &lt;- as.formula(paste0(\"y ~ \", indics, \"| unit + year | 0 | unit\"))\n\n# estimate the model and make the plot\ndata %&gt;% \n  # make pre and post variables\n  mutate(Pre = if_else(!is.na(rel_year) & rel_year &lt; 10, 1, 0),\n         Post = if_else(!is.na(rel_year) & rel_year &gt; 10, 1, 0)) %&gt;%\n  # generate the outcome variable\n  mutate(y = unit_fe + year_fe + cumtau + error) %&gt;% \n  # run the model and tidy up\n  do(broom::tidy(felm(form, data = .), conf.int = TRUE)) %&gt;%\n  filter(!(term %in% c(\"firm_age\", \"Pre\", \"Post\"))) %&gt;%\n  # make the relative time variable and keep just what we need\n  mutate(time = c(-10:(-2), 0:10)) %&gt;% \n  select(time, estimate, conf.low, conf.high) %&gt;% \n  # bring back in missing indicator for t = -1\n  bind_rows(tibble(\n    time = -1, estimate = 0, conf.low = 0, conf.high = 0\n  )) %&gt;% \n  # keep just -10 to 10\n  filter(time %&gt;% between(-10, 10)) %&gt;% \n  left_join(true_taus) %&gt;% \n  # split the error bands by pre-post\n  mutate(band_groups = case_when(\n    time &lt; -1 ~ \"Pre\",\n    time &gt;= 0 ~ \"Post\",\n    time == -1 ~ \"\"\n  )) %&gt;%\n  # plot it\n  ggplot(aes(x = time, y = estimate)) + \n  geom_line(aes(x = time, y = true_tau, color = \"True Effect\"), size = 1.5, linetype = \"dashed\") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, group = band_groups),\n              color = \"lightgrey\", alpha = 1/4) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = \"Estimated Effect\"), \n                  show.legend = FALSE) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  scale_x_continuous(breaks = seq(-10, 10, by = 2)) + \n  labs(x = \"Relative Time\", y = \"Estimate\") +\n  scale_color_brewer(palette = 'Set1') + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16))\n\n\n\n\n\n\n\n\n\nWhoops! That’s no good. Try messing around with different forms of this, either only putting in the relative time indicators (thus leaving out both Pre and Post) or dropping one or the other, and you’ll see that things get really wonky. It depends on your DGP but we should all be fully saturating the relative time indicators every time.\nMoving on, what if you were to erroneously control for log age even though it isn’t related to either the outcome or the treatment? We’re still good (this should be obvious, but it’s nice to think through these relations and prove it to yourself).\n\n\nCode\ntheme_set(theme_clean() + theme(plot.background = element_blank()))\n\n# make string for rel time indicators \nindics &lt;- paste0(c(paste0(\"`lag_\", seq(min, -2), \"`\"),\n                   paste0(\"lag_\", seq(0, max))),\n                 collapse = \" + \")\n\n# make the formula we want to run\nform &lt;- as.formula(paste0(\"y ~ \", indics, \" + log_age | unit + year | 0 | unit\"))\n\n# estimate the model and make the plot\ndata %&gt;% \n  # generate the outcome variable\n  mutate(y = unit_fe + year_fe + cumtau + error) %&gt;% \n  # run the model and tidy up\n  do(broom::tidy(felm(form, data = .), conf.int = TRUE)) %&gt;%\n  filter(term != \"log_age\") %&gt;% \n  # make the relative time variable and keep just what we need\n  mutate(time = c(min:(-2), 0:max)) %&gt;% \n  select(time, estimate, conf.low, conf.high) %&gt;% \n  # bring back in missing indicator for t = -1\n  bind_rows(tibble(\n    time = -1, estimate = 0, conf.low = 0, conf.high = 0\n  )) %&gt;% \n  # keep just -10 to 10\n  filter(time %&gt;% between(-10, 10)) %&gt;% \n  left_join(true_taus) %&gt;% \n  # split the error bands by pre-post\n  mutate(band_groups = case_when(\n    time &lt; -1 ~ \"Pre\",\n    time &gt;= 0 ~ \"Post\",\n    time == -1 ~ \"\"\n  )) %&gt;%\n  # plot it\n  ggplot(aes(x = time, y = estimate)) + \n  geom_line(aes(x = time, y = true_tau, color = \"True Effect\"), size = 1.5, linetype = \"dashed\") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, group = band_groups),\n              color = \"lightgrey\", alpha = 1/4) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = \"Estimated Effect\"), \n                  show.legend = FALSE) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  scale_x_continuous(breaks = seq(-10, 10, by = 2)) + \n  labs(x = \"Relative Time\", y = \"Estimate\") +\n  scale_color_brewer(palette = 'Set1') + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16))\n\n\n\n\n\n\n\n\n\nOkay, let’s start to make things more interesting. First, what if our log_age variable is associated with our outcome variable, but not with the treatment. Now I change the DGP to be:\n\\[y_{it} = \\tag{1} \\alpha_i + \\alpha_t + \\sum_{t = min + 1, t \\neq -1}^{max} \\gamma_i \\cdot D_{it}  - 0.85 \\cdot log\\_age + \\epsilon\\]\n\n\nCode\ndagify(y ~ t,\n       y ~ x,\n       labels = c(\"y\" = \"Outcome\",\n                  \"t\" = \"Treatment\",\n                  \"x\" = \"Log of Firm Age\")) %&gt;% \n  ggdag(text = FALSE, use_labels = \"label\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nIf we run the regression controlling and control for log_age, we’re good. (Note, you shouldn’t need to control for log_age here because it isn’t a confounding variable - it only impacts the outcome variable. But, there are weird time dynamics that intersect with our relative period dummies and the omitted variables - essentially firm age is associated with the relative time dummies because time only goes in one direction and so does treatment assignment. Too tired to think through the consequences here, but with this DGP you do need to control for log_age to exactly hit the estimates with TWFE).\n\n\nCode\ntheme_set(theme_clean() + theme(plot.background = element_blank()))\n\n# next, assume that there is an effect on the outcome variable but no effect on the treatment outcome\n# make string for rel time indicators \nindics &lt;- paste0(c(paste0(\"`lag_\", seq(min, -2), \"`\"),\n                   paste0(\"lag_\", seq(0, max))),\n                 collapse = \" + \")\n\nform &lt;- as.formula(paste0(\"y ~ \", indics, \"  + log_age| unit + year | 0 | unit\"))\n\n# estimate the model and make the plot\ndata %&gt;% \n  # generate the outcome variable\n  mutate(y = unit_fe + year_fe + cumtau + -.85*log_age + error) %&gt;% \n  # run the model and tidy up\n  do(broom::tidy(felm(form, data = .), conf.int = TRUE)) %&gt;% \n  # make the relative time variable and keep just what we need\n  filter(term != \"log_age\") %&gt;% \n  mutate(time = c(min:(-2), 0:max)) %&gt;% \n  select(time, estimate, conf.low, conf.high) %&gt;% \n  # bring back in missing indicator for t = -1\n  # bring back in missing indicator for t = -1\n  bind_rows(tibble(\n    time = -1, estimate = 0, conf.low = 0, conf.high = 0\n  )) %&gt;% \n  # keep just -10 to 10\n  filter(time %&gt;% between(-10, 10)) %&gt;% \n  left_join(true_taus) %&gt;% \n  # split the error bands by pre-post\n  mutate(band_groups = case_when(\n    time &lt; -1 ~ \"Pre\",\n    time &gt;= 0 ~ \"Post\",\n    time == -1 ~ \"\"\n  )) %&gt;%\n  # plot it\n  ggplot(aes(x = time, y = estimate)) + \n  geom_line(aes(x = time, y = true_tau, color = \"True Effect\"), size = 1.5, linetype = \"dashed\") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, group = band_groups),\n              color = \"lightgrey\", alpha = 1/4) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = \"Estimated Effect\"), \n                  show.legend = FALSE) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  scale_x_continuous(breaks = seq(-10, 10, by = 2)) + \n  labs(x = \"Relative Time\", y = \"Estimate\") +\n  scale_color_brewer(palette = 'Set1') + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16)) \n\n\n\n\n\n\n\n\n\nFinally, what if log_age is a confounding variable? That is, it impacts both the treatment and outcome variable and needs to be controlled for under causal-inference-dag-circle-drawing dogma:\n\n\nCode\ndagify(y ~ t,\n       y ~ x,\n       t ~ x,\n       labels = c(\"y\" = \"Outcome\",\n                  \"t\" = \"Treatment\",\n                  \"x\" = \"Log of Firm Age\")) %&gt;% \n  ggdag(text = FALSE, use_labels = \"label\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nI implement this somewhat cludgely, but essentially I simulate the data in the same manner as above, but now the treatment assignment is in part determined by firm age. For each firm I get their average age in the panel and use that as an inverse weight when I randomly assign the treatment to firms (probably better ways to do this but it works). Now, not only is the estimate still valid when controlling for log_age but you in fact need to control for it to get unbiased estimates.\n\n\nCode\ntheme_set(theme_clean() + theme(plot.background = element_blank()))\n\n# Now assume that the log age variable is correlated in some way with the treatment assignment decision\n# in particular assume that younger firms are more likely to get targeted \n\n# Generate data - 20,000 firms are placed in each group. Groups 3 and 4 are \n# treated in 1998, Groups 1 and 2 are untreated\n\nmake_data &lt;- function(...) {\n  \n  # Fixed Effects ------------------------------------------------\n  # unit fixed effects\n  unit &lt;- tibble(\n    unit = 1:20000, \n    unit_fe = rnorm(20000, 0, 1)) %&gt;%\n    # make first and last year per unit, and treat year if treated\n    rowwise() %&gt;% \n    mutate(first_year = sample(seq(1981, 2010), 1),\n           # pull last year as a randomly selected date bw first and 2010\n           last_year = if_else(first_year &lt; 2010, sample(seq(first_year, 2010), 1), \n                               as.integer(2010))) %&gt;% \n    ungroup()\n  \n  # year fixed effects \n  year &lt;- tibble(\n    year = 1981:2010,\n    year_fe = rnorm(30, 0, 1))\n  \n  # make panel\n  data &lt;- crossing(unit, year) %&gt;% \n    arrange(unit, year) %&gt;% \n    # keep only if year between first and last year \n    rowwise() %&gt;% \n    filter(year %&gt;% between(first_year, last_year)) %&gt;% \n    ungroup() %&gt;% \n    mutate(firm_age = year - first_year)\n  \n    # make an average age data frame \n  avg_age &lt;- data %&gt;% \n    group_by(unit) %&gt;% \n    summarize(avg_age = mean(firm_age)) %&gt;% \n    mutate(weight = 1 / (avg_age + 1))\n  \n  # sample 4,000 firms to get treatment, weighted by average age\n  treated_units &lt;- sample_n(avg_age, 4000, replace = FALSE, weight = weight) %&gt;%\n    mutate(treat = 1) %&gt;% \n    select(unit, treat)\n  \n  # merge treat back into the data \n  treat_data &lt;- data %&gt;% \n    select(unit, first_year, last_year) %&gt;% \n    distinct() %&gt;% \n    left_join(treated_units) %&gt;% \n    replace_na(list(treat = 0)) %&gt;% \n    rowwise() %&gt;% \n    mutate(treat_year = if_else(treat == 1,\n                                if_else(first_year != last_year,\n                                        sample(first_year:last_year, 1), as.integer(first_year)),\n                                as.integer(0)))\n  \n  # merge back into main data\n  data &lt;- data %&gt;% \n    left_join(treat_data) %&gt;% \n    # make error term, treat term and log age term\n    mutate(error = rnorm(nrow(.), 0, 1),\n           posttreat = if_else(treat == 1 & year &gt;= treat_year, 1, 0),\n           rel_year = if_else(treat == 1, year - treat_year, as.integer(NA)),\n           tau = if_else(posttreat == 1, .2, 0),\n           firm_age = year - first_year,\n           log_age = log(firm_age + 1)) %&gt;% \n    # make cumulative treatment effects\n    group_by(unit) %&gt;% \n    mutate(cumtau = cumsum(tau)) %&gt;% \n    ungroup() %&gt;% \n    # finally, make dummy variables\n    bind_cols(dummy_cols(tibble(lag = .$rel_year), select_columns = \"lag\", \n                         ignore_na = TRUE, remove_selected_columns = TRUE)) %&gt;%\n    # replace equal to 0 for all lead lag columns\n    mutate_at(vars(starts_with(\"lag_\")), ~replace_na(., 0))\n}\n\n# make data\ndata2 &lt;- make_data()\n\n# get var names in a vector - need to drop the most negative lag (lag_1) and \nmin &lt;- min(data2$rel_year, na.rm = TRUE) + 1\nmax &lt;- max(data2$rel_year, na.rm = TRUE)\n\n# make string for rel time indicators \nindics &lt;- paste0(c(paste0(\"`lag_\", seq(min, -2), \"`\"),\n                   paste0(\"lag_\", seq(0, max))),\n                 collapse = \" + \")\n\n# make the formula we want to run\nform1 &lt;- as.formula(paste0(\"y ~ \", indics, \"| unit + year | 0 | unit\"))\nform2 &lt;- as.formula(paste0(\"y ~ \", indics, \" + log_age| unit + year | 0 | unit\"))\n\n# estimate the model and make the plot\ndata2 %&gt;% \n  # generate the outcome variable\n  mutate(y = unit_fe + year_fe + cumtau + -.85*log_age + error) %&gt;% \n  # run the model and tidy up\n  do(bind_rows(\n    broom::tidy(felm(form1, data = .), conf.int = TRUE) %&gt;% mutate(mod = \"No Control Log Age\"),\n    broom::tidy(felm(form2, data = .), conf.int = TRUE) %&gt;% mutate(mod = \"Control Log Age\"))) %&gt;% \n  # make the relative time variable and keep just what we need\n  filter(term != \"log_age\") %&gt;% \n  mutate(time = rep(c(min:(-2), 0:max), 2)) %&gt;% \n  select(time, estimate, conf.low, conf.high, mod) %&gt;% \n  # bring back in missing indicator for t = -1\n  bind_rows(tibble(\n    time = rep(-1, 2), estimate = rep(0, 2), conf.low = rep(0, 2),\n    conf.high = rep(0, 2), mod = c(\"No Control Log Age\", \"Control Log Age\")\n  )) %&gt;% \n  # keep just -10 to 10\n  filter(time %&gt;% between(-10, 10)) %&gt;% \n  left_join(true_taus) %&gt;% \n  # split the error bands by pre-post\n  mutate(band_groups = case_when(\n    time &lt; -1 ~ \"Pre\",\n    time &gt;= 0 ~ \"Post\",\n    time == -1 ~ \"\"\n  )) %&gt;%\n  # plot it\n  ggplot(aes(x = time, y = estimate)) + \n  geom_line(aes(x = time, y = true_tau, color = \"True Effect\"), size = 1.5, linetype = \"dashed\") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, group = band_groups),\n              color = \"lightgrey\", alpha = 1/4) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = \"Estimated Effect\"), \n                  show.legend = FALSE) + \n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") + \n  scale_x_continuous(breaks = seq(-10, 10, by = 2)) + \n  labs(x = \"Relative Time\", y = \"Estimate\") +\n  scale_color_brewer(palette = 'Set1') + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16),\n        panel.spacing = unit(2, \"lines\")) + \n  facet_wrap(~mod)\n\n\n\n\n\n\n\n\n\nIn conclusion, at least in this very straightforward example, you do need to control for the log of firm age, even though firm age itself is collinear with the fixed effects structure. Mea culpa."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrew C. Baker",
    "section": "",
    "text": "I am an Assistant Professor at Berkeley Law School, and a recent PhD and law graduate from Stanford University. My research interests include corporate governance, empirical legal studies, and law and economics/finance."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Andrew C. Baker",
    "section": "Education",
    "text": "Education\nStanford University | Stanford, CA\nPhD in Business Administration | 2021\nStanford University | Stanford, CA\nJuris Doctor | 2017\nGeorgetown University| Washington, DC\nBSc in International Economics | 2009"
  }
]